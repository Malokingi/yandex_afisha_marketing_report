{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll put the bulk of the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def check_data(df):\n",
    "    df.info()\n",
    "    print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "First let's tidy up the data we were given (See `datawasher.ipynb` for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Visits Table\n",
    "file_path = 'datasets_dirty/visits_log_us.csv'\n",
    "\n",
    "try:\n",
    "    open(file_path, 'r')\n",
    "except FileNotFoundError:\n",
    "    file_path = '/datasets/visits_log_us.csv'\n",
    "\n",
    "v_df = pd.read_csv(\n",
    "    file_path,\n",
    "    parse_dates=['Start Ts', 'End Ts'],\n",
    "    dtype=\n",
    "        {\n",
    "            'Device': 'category',\n",
    "            'Source Id': 'category'\n",
    "        }\n",
    ")\n",
    "\n",
    "v_df = v_df.rename(\n",
    "    columns={\n",
    "        'Uid': 'uid',\n",
    "        'Device': 'device',\n",
    "        'Start Ts': 'start_time',\n",
    "        'End Ts': 'end_time',\n",
    "        'Source Id': 'source_id'\n",
    "    }\n",
    ")\n",
    "check_data(v_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going from memory usage: 79.3 MB to memory usage: 8.9 MB without any loss of data? Nice.\n",
    "## The visits table (server logs with data on website visits):\n",
    "- uid — user's unique identifier\n",
    "    - Change from 'Uid' to 'uid'\n",
    "- device — user's device\n",
    "    - Change from 'Device' to 'device'\n",
    "    - There's only two different values, so I'll change the type to category\n",
    "- start_time — session start date and time\n",
    "    - Change name from 'Start Ts' to 'start_time'\n",
    "    - Looks like the seconds aren't included in this, I'll convert to datetime\n",
    "- end_time — session end date and time\n",
    "    - Change name from 'End Ts' to 'end_time'\n",
    "    - Change to datetime type also\n",
    "- source_id — identifier of the ad source the user came from\n",
    "    - Change name from 'Source Id' to 'source_id'\n",
    "    - There's only 10 unique values, so I changed this to category type. I'll come back and undo if I need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Orders Table\n",
    "file_path = 'datasets_dirty/orders_log_us.csv'\n",
    "\n",
    "try:\n",
    "    open(file_path, 'r')\n",
    "except FileNotFoundError:\n",
    "    file_path = '/datasets/orders_log_us.csv'\n",
    "\n",
    "o_df = pd.read_csv(\n",
    "    file_path,\n",
    "    parse_dates=['Buy Ts']\n",
    ")\n",
    "\n",
    "o_df = o_df.rename(\n",
    "    columns={\n",
    "        'Uid': 'uid',\n",
    "        'Buy Ts': 'purchase_time',\n",
    "        'Revenue': 'profit'\n",
    "    }\n",
    ")\n",
    "check_data(o_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going from memory usage: 4.4 MB to memory usage: 1.2 MB without any loss of data? Nice.\n",
    "## The orders table (data on orders):\n",
    "- uid — unique identifier of the user making an order\n",
    "    - Change from 'Uid' to 'uid'\n",
    "- purchase_time — order date and time\n",
    "    - Change from 'Buy Ts' to 'purchase_time'\n",
    "    - Convert to datetime type\n",
    "- profit — Yandex.Afisha's revenue from the order\n",
    "    - Change from 'Revenue' to 'profit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Costs Table\n",
    "file_path = 'datasets_dirty/costs_us.csv'\n",
    "\n",
    "try:\n",
    "    open(file_path, 'r')\n",
    "except FileNotFoundError:\n",
    "    file_path = '/datasets/costs_us.csv'\n",
    "\n",
    "c_df = pd.read_csv(\n",
    "    file_path,\n",
    "    parse_dates=['dt'],\n",
    "    dtype=\n",
    "        {\n",
    "            'Device': 'category',\n",
    "            'source_id': 'category'\n",
    "        }\n",
    ")\n",
    "\n",
    "c_df = c_df.rename(\n",
    "    columns={\n",
    "        'dt': 'date'\n",
    "    }\n",
    ")\n",
    "check_data(c_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going from memory usage: 206.2 KB to memory usage: 42.7 KB without any loss of data? Stellar move by me.\n",
    "## The costs table (data on marketing expenses):\n",
    "- source_id — ad source identifier\n",
    "    - There's only 7 unique values. Convert to category type\n",
    "- dt — date\n",
    "    - change from 'dt' to 'date'\n",
    "    - It only has dates, and no times. Convert to datetime type accordingly\n",
    "- costs — expenses on this ad source on this day\n",
    "    - This looks fine unchanged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "```\n",
    "Q: How many people use this every day, week, and month?\n",
    "\n",
    "A:  Monthly average users =     23,228\n",
    "    Weekly average users =      5,716\n",
    "    Daily average users =       907 (about 15.88% of the weekly and 3.91% of the monthly users)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns indicating the year, month, week, and day of a users `start_time`\n",
    "v_df['session_year']  = pd.to_datetime(v_df['start_time'].dt.isocalendar().year)\n",
    "v_df['session_month'] = pd.to_datetime(v_df['start_time'].dt.to_period('M').dt.to_timestamp()) # Retains year and month\n",
    "v_df['session_week'] = v_df['start_time'].dt.strftime('%Y-%U')  # Retains year and week\n",
    "v_df['session_date'] = pd.to_datetime(v_df['start_time'].dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average daily, weekly, and monthly users\n",
    "dau_total = (\n",
    "    v_df.groupby('session_date')\n",
    "    .agg({'uid': 'nunique'})\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "wau_total = (\n",
    "    v_df.groupby(['session_year', 'session_week'])\n",
    "    .agg({'uid': 'nunique'})\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "mau_total = (\n",
    "    v_df.groupby(['session_year', 'session_month'])\n",
    "    .agg({'uid': 'nunique'})\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# Also, sticky versions\n",
    "sticky_wau = (dau_total / wau_total) * 100\n",
    "sticky_mau = (dau_total / mau_total) * 100\n",
    "\n",
    "print(f'dau = {int(dau_total)} wau = {int(wau_total)} mau = {int(mau_total)}')\n",
    "print(f'sticky_wau = {float(sticky_wau):.2f}% sticky_mau = {float(sticky_mau):.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: What is the length of each session?\n",
    "\n",
    "A: Average Session length = 10.73mins\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding session duration in seconds, then dividing by 60 to get mins, then print the average\n",
    "v_df['session_duration_mins'] = (\n",
    "    v_df['end_time'] - v_df['start_time']\n",
    ").dt.seconds / 60\n",
    "\n",
    "print(f\"Average Session length = {float(v_df['session_duration_mins'].mean()):.2f}mins\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: How many sessions are there per day?\n",
    "\n",
    "A: Average Daily Sessions = 987.36 sessions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of sessions per day, then printing the average\n",
    "v_by_day = v_df.groupby('session_date').agg({\n",
    "    'start_time': 'count'\n",
    "})\n",
    "v_by_day = v_by_day.rename(\n",
    "    columns={\n",
    "        'start_time': 'sessions'\n",
    "    }\n",
    ")\n",
    "print(f\"Average Daily Sessions = {float(v_by_day['sessions'].mean()):.2f} sessions\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: What's the user retention rate?\n",
    "\n",
    "A: See the Pivot Table below\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating first session date to be used for other cohort calculations\n",
    "first_session_date = v_df.groupby(['uid'])['session_date'].min()\n",
    "first_session_date.name = 'first_session_date'\n",
    "\n",
    "v_df = v_df.join(first_session_date, on='uid')\n",
    "\n",
    "# For daily cohorts, if we want to later\n",
    "\n",
    "# v_df['cohort_lifetime_days'] = ((\n",
    "#         v_df['session_date']\n",
    "#             -\n",
    "#         v_df['first_session_date']\n",
    "#     ) / np.timedelta64(1, 'D')\n",
    "# ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For weekly cohorts, if we want to later\n",
    "\n",
    "# first_session_week = v_df.groupby(['uid'])['session_week'].min()\n",
    "# first_session_week.name = 'first_session_week'\n",
    "\n",
    "# v_df = v_df.join(first_session_week, on='uid')\n",
    "\n",
    "# v_df['cohort_lifetime_weeks'] = ((\n",
    "#         v_df['session_date']\n",
    "#             -\n",
    "#         v_df['first_session_date']\n",
    "#     ) / np.timedelta64(1, 'W')\n",
    "# ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Monthly Cohort Lifetime in Months\n",
    "\n",
    "first_session_month = v_df.groupby(['uid'])['session_month'].min()\n",
    "first_session_month.name = 'first_session_month'\n",
    "\n",
    "try:\n",
    "    v_df = v_df.join(first_session_month, on='uid')\n",
    "except:\n",
    "    print('we already have first_session_month')\n",
    "\n",
    "v_df['cohort_lifetime_months'] = (\n",
    "    (\n",
    "        v_df['session_date'].astype('datetime64[M]')\n",
    "            -\n",
    "        v_df['first_session_date'].astype('datetime64[M]')\n",
    "    ) / np.timedelta64(1, 'M')\n",
    ").astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by Cohort\n",
    "cohorts_monthly = (\n",
    "    v_df.groupby(['first_session_month', 'cohort_lifetime_months'])\n",
    "    .agg({'uid': 'nunique'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "initial_users_count = cohorts_monthly[cohorts_monthly['cohort_lifetime_months'] == 0][ ['first_session_month', 'uid'] ]\n",
    "initial_users_count = initial_users_count.rename(\n",
    "    columns={'uid': 'monthly_cohort_users'}\n",
    ") \n",
    "\n",
    "cohorts_monthly = cohorts_monthly.merge(initial_users_count, on='first_session_month')\n",
    "\n",
    "# Add retention rate column\n",
    "cohorts_monthly['retention'] = cohorts_monthly['uid'] / cohorts_monthly['monthly_cohort_users']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make and print table\n",
    "retention_pivot = cohorts_monthly.pivot_table(\n",
    "    index='first_session_month',\n",
    "    columns='cohort_lifetime_months',\n",
    "    values='retention',\n",
    "    aggfunc='sum'\n",
    ")\n",
    "\n",
    "# Format the percents\n",
    "retention_pivot = retention_pivot.applymap(lambda x: '{:.2%}'.format(x))\n",
    "\n",
    "# Get rid og the nan% values\n",
    "retention_pivot = retention_pivot.replace('nan%', '')\n",
    "\n",
    "retention_pivot\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: When do users start buying?\n",
    "\n",
    "A: Average Conversion Rate = 16.90 days\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the earliest purchase date for each uid (aka for each user)\n",
    "first_purchase_date = o_df.groupby(['uid'])['purchase_time'].min()\n",
    "first_purchase_date.name = 'first_purchase_date'\n",
    "\n",
    "user_conversion = (\n",
    "    first_session_date.to_frame()\n",
    "    .join(first_purchase_date, on='uid')\n",
    "    .dropna()\n",
    "    # .fillna('never')\n",
    ")\n",
    "user_conversion.reset_index(inplace=True)\n",
    "\n",
    "# Add number of days between users first session and first purchase\n",
    "user_conversion['conversion_days'] = ((\n",
    "        user_conversion['first_purchase_date']\n",
    "            -\n",
    "        pd.to_datetime(user_conversion['first_session_date'])\n",
    "    ) / np.timedelta64(1, 'D')\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_conversion['conversion_days'].value_counts()\n",
    "print(f\"Average Conversion Rate = {float(user_conversion['conversion_days'].mean()):.2f} days\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: How many orders do users make during a given period of time?\n",
    "\n",
    "A: Average Number of Orders per Month = 1.27 orders\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_df = o_df.merge(user_conversion[['uid', 'first_purchase_date']], on='uid', how='left')\n",
    "\n",
    "o_df['user_age'] = ((\n",
    "        o_df['purchase_time']\n",
    "            -\n",
    "        o_df['first_purchase_date']\n",
    "    ) / np.timedelta64(1, 'D')\n",
    ")\n",
    "\n",
    "# Group o_df by uid and calculate the total number of purchases per uid\n",
    "purchase_count = o_df.groupby('uid')['purchase_time'].count()\n",
    "\n",
    "# Calculate the duration in days, weeks, and months for each uid\n",
    "duration_days = (o_df.groupby('uid')['purchase_time'].max() - o_df.groupby('uid')['purchase_time'].min()).dt.days\n",
    "\n",
    "duration_months = duration_days / 30\n",
    "\n",
    "# Calculate the average number of purchases per month\n",
    "avg_purchases_per_month = np.where(duration_months == 0, 0, purchase_count / duration_months)\n",
    "\n",
    "# Combine the results into a new DataFrame\n",
    "average_purchases_df = pd.DataFrame({\n",
    "    'uid': purchase_count.index,\n",
    "    'avg_purchases_per_month': avg_purchases_per_month\n",
    "})\n",
    "\n",
    "# Display the average purchases per month\n",
    "print(f\"Average Number of Orders per Month = {float(average_purchases_df['avg_purchases_per_month'].mean()):.2f} orders\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: What is the average purchase size?\n",
    "\n",
    "A: Average Size of Orders per User = 4.09 units of currency\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group o_df by uid and calculate the total profit and number of purchases per uid\n",
    "purchase_stats = o_df.groupby('uid').agg({'profit': 'sum', 'purchase_time': 'count'})\n",
    "\n",
    "# Calculate the average purchase size per user\n",
    "average_purchase_size_per_user = purchase_stats['profit'] / purchase_stats['purchase_time']\n",
    "\n",
    "# Add the average purchase size per user to o_df\n",
    "o_df = o_df.merge(average_purchase_size_per_user.rename('average_purchase_size_per_user'), on='uid')\n",
    "\n",
    "# Display the average purchase size per user\n",
    "average_purchase_size_per_user_df = o_df[['uid', 'average_purchase_size_per_user']].drop_duplicates()\n",
    "\n",
    "print(f\"Average Size of Orders per User {float(average_purchase_size_per_user_df['average_purchase_size_per_user'].mean()):.2f} units of currency\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: How much money do users bring?\n",
    "\n",
    "A: Average Lifetime Value per User = 6.90 units of currency\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ltv_data = o_df.groupby('uid')['profit'].sum().reset_index()\n",
    "\n",
    "total_ltv_data = total_ltv_data.sort_values('profit', ascending=False)\n",
    "\n",
    "# total_ltv_data.head(20)\n",
    "print(f\"Average LTV per User = {float(total_ltv_data['profit'].mean()):.2f} units of currency\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: How much money was spent overall?\n",
    "\n",
    "A: Total Overall Cost = 329131.62 units of currency\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_costs = c_df.agg({'costs': 'sum'})\n",
    "print(f\"Total Overall Cost = {float(total_costs):.2f} units of currency\")\n",
    "# total_costs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: How much money was spent per source?\n",
    "\n",
    "A:  See the bar chart below\n",
    "    Total Cost for source 3 =   141,321.63 units of currency, the highest by a large margin\n",
    "\n",
    "    Total Cost for source 10 =  5,822.49 units of currency\n",
    "    Total Cost for source 9 =   5,517.49 units of currency\n",
    "    \n",
    "    Sources 9 and 10 seem to be the least expensive, let's see how effective these sources are next.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcely_costs = c_df.groupby('source_id').agg({'costs':'sum'}).reset_index().sort_values('costs', ascending=False)\n",
    "\n",
    "# Plotting the total costs as a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sourcely_costs['source_id'], sourcely_costs['costs'])\n",
    "plt.xlabel('Source ID')\n",
    "plt.ylabel('Total Costs')\n",
    "plt.title('Total Costs by Source ID')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add the value to each bar as text\n",
    "for i, value in enumerate(sourcely_costs['costs']):\n",
    "    plt.text(i, value, f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: How much money was spent over time?\n",
    "\n",
    "A: See the Line Graph below:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_costs = c_df.groupby('date').agg({'costs': 'sum'})\n",
    "\n",
    "# Create the plot using Pandas plot()\n",
    "daily_costs.plot(figsize=(10, 6))\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Costs by Date')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Costs')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: How much did customer acquisition from each of the sources cost?\n",
    "\n",
    "A: See the Bar Chart below:\n",
    "    The highest CAC seems to be 1.89 units of currency for Source_id = 3\n",
    "    The most efficent CACs seem to go to 9, 10, and 4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcely_users = (\n",
    "    v_df.groupby(['source_id'])\n",
    "    .agg({'uid': 'nunique'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "sourcely_users.columns = ['source_id', 'n_users']\n",
    "\n",
    "cac_report = pd.merge(\n",
    "    sourcely_costs,\n",
    "    sourcely_users,\n",
    "    on='source_id'\n",
    ")\n",
    "\n",
    "cac_report['cac'] = cac_report['costs'] / cac_report['n_users']\n",
    "# sourcely_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by average CAC in descending order\n",
    "cac_report_sorted = cac_report.sort_values('cac', ascending=False)\n",
    "\n",
    "# Plotting the average CAC\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(cac_report_sorted['source_id'], cac_report_sorted['cac'])\n",
    "plt.xlabel('Source ID')\n",
    "plt.ylabel('Average CAC')\n",
    "plt.title('Average Customer Acquisition Cost by Source ID')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add the value to each bar as text\n",
    "for i, value in enumerate(cac_report_sorted['cac']):\n",
    "    plt.text(i, value, f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: How worthwhile were the investments?\n",
    "\n",
    "A: Total Average Return on Investment per User = 4.78 units of currency\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average lifetime value using our table from earlier\n",
    "avg_ltv_per_user = total_ltv_data.agg({'profit':'mean'})\n",
    "\n",
    "# Count the total number of unique users\n",
    "total_n_users = v_df.agg({'uid': 'nunique'})\n",
    "\n",
    "# Find the average Acquision cost per user\n",
    "average_cac_per_user = total_costs[0] / total_n_users[0]\n",
    "\n",
    "# Calculate Return on Investment\n",
    "roi = avg_ltv_per_user / average_cac_per_user\n",
    "\n",
    "print(f\"Total Average Return on Investment per User = {float(roi):.2f} units of currency\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Next I'll show how the CAC, LTV, and ROI change depending on Ad Source and on Device\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph CAC, LTV, and ROI per Source Id per week\n",
    "\n",
    "# Calculate CAC per source_id and time period\n",
    "cac_per_source = c_df.groupby(['source_id', pd.Grouper(key='date', freq='W-SUN')]).agg({'costs': 'sum'}).reset_index()\n",
    "\n",
    "# Calculate LTV per source_id and time period\n",
    "# Merge visits and orders tables on 'uid' column\n",
    "v_and_o_dfs = v_df.merge(o_df, on='uid')\n",
    "\n",
    "# Group merged dataframe by 'source_id' and week of 'purchase_time', calculate LTV\n",
    "sourcely_ltv = v_and_o_dfs.groupby(['source_id', pd.Grouper(key='purchase_time', freq='W-SUN')]).agg(\n",
    "    ltv=('profit', 'sum'),\n",
    "    unique_users=('uid', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate ROI per source_id and time period\n",
    "roi_per_source = sourcely_ltv.copy()\n",
    "roi_per_source['roi'] = roi_per_source['ltv'] / cac_per_source['costs']\n",
    "\n",
    "# Plotting CAC per source_id\n",
    "plt.figure(figsize=(10, 6))\n",
    "for source_id, group in cac_per_source.groupby('source_id'):\n",
    "    plt.plot(group['date'], group['costs'], label=f\"Source {source_id}\")\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('CAC')\n",
    "plt.title('CAC per Source over Time (Weekly)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting LTV per source_id\n",
    "plt.figure(figsize=(10, 6))\n",
    "for source_id, group in sourcely_ltv.groupby('source_id'):\n",
    "    plt.plot(group['purchase_time'], group['ltv'], label=f\"Source {source_id}\")\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('LTV')\n",
    "plt.title('LTV per Source over Time (Weekly)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting ROI per source_id\n",
    "plt.figure(figsize=(10, 6))\n",
    "for source_id, group in roi_per_source.groupby('source_id'):\n",
    "    plt.plot(group['purchase_time'], group['roi'], label=f\"Source {source_id}\")\n",
    "    last_data_point = group.iloc[-1]  # Get the last data point for each source_id\n",
    "    plt.annotate(f\"Source {source_id}\", xy=(last_data_point['purchase_time'], last_data_point['roi']), xytext=(10, -10),\n",
    "                 textcoords='offset points', ha='left', va='top', color='black', fontsize=8)\n",
    "\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('ROI')\n",
    "plt.title('ROI per Source over Time (Weekly)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "We see that CAC for Source 3 is much higher than all the others pretty consistently and might want to reconsider our partnership with them.\n",
    "\n",
    "In the second Graph, we see that Source 2 and 5 had pretty significant spikes in LTVs around the end of December. Whatever they were doing was working.\n",
    "\n",
    "Finally in the third graph we see that Source 1 was most consistently the highest performer. We also see that 10, 3, 4, 6, 7, and 9 are practically flatlining and might want to reconsider them as well.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph CAC, LTV, and ROI per device per week\n",
    "\n",
    "# Extract the date part from the \"end_time\" column in v_df\n",
    "v_df['date'] = pd.to_datetime(v_df['end_time'].dt.date)\n",
    "\n",
    "# Merge v_df and c_df based on the date part\n",
    "v_and_c_dfs = v_df.merge(c_df, on='date')\n",
    "\n",
    "# Calculate CAC per device and week\n",
    "cac_per_device_week = v_and_c_dfs.groupby([v_and_c_dfs['device'], pd.Grouper(key='date', freq='W-MON')])['costs'].sum().reset_index()\n",
    "\n",
    "# Calculate LTV per device and week\n",
    "ltv_per_device_week = v_and_o_dfs.groupby([v_and_o_dfs['device'], pd.Grouper(key='purchase_time', freq='W-SUN')])['profit'].sum().reset_index()\n",
    "\n",
    "# Calculate ROI per device and week\n",
    "roi_per_device_week = ltv_per_device_week.copy()\n",
    "roi_per_device_week['roi'] = roi_per_device_week['profit'] / cac_per_device_week['costs']\n",
    "\n",
    "# Plotting CAC per device\n",
    "plt.figure(figsize=(10, 6))\n",
    "for device, group in cac_per_device_week.groupby('device'):\n",
    "    plt.plot(group['date'], group['costs'], label=device)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('CAC')\n",
    "plt.title('CAC per Device over Time (Weekly)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting LTV per device\n",
    "plt.figure(figsize=(10, 6))\n",
    "for device, group in ltv_per_device_week.groupby('device'):\n",
    "    plt.plot(group['purchase_time'], group['profit'], label=device)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('LTV')\n",
    "plt.title('LTV per Device over Time (Weekly)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting ROI per device\n",
    "plt.figure(figsize=(10, 6))\n",
    "for device, group in roi_per_device_week.groupby('device'):\n",
    "    plt.plot(group['purchase_time'], group['roi'], label=device)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('ROI')\n",
    "plt.title('ROI per Device over Time (Weekly)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "At first we see that the CAC for Desktop users is much higher, but in the nest graph we see that the returns are also much higher. \n",
    "\n",
    "In the bottom graph, we see indeed that the higher profits for Desktop make up for the higher costs and indeed outperforms touch devices almost every week out of the year.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "And Questions or concerns or tyops you'd like to being to my attention, please eMail me whenever is convenient for you.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
