{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll put the bulk of the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def check_data(df):\n",
    "    df.info()\n",
    "    print(df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "First let's tidy up the data we were given (See `datawasher.ipynb` for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 359400 entries, 0 to 359399\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count   Dtype         \n",
      "---  ------      --------------   -----         \n",
      " 0   device      359400 non-null  category      \n",
      " 1   end_time    359400 non-null  datetime64[ns]\n",
      " 2   source_id   359400 non-null  category      \n",
      " 3   start_time  359400 non-null  datetime64[ns]\n",
      " 4   uid         359400 non-null  uint64        \n",
      "dtypes: category(2), datetime64[ns](2), uint64(1)\n",
      "memory usage: 8.9 MB\n",
      "    device            end_time source_id          start_time  \\\n",
      "0    touch 2017-12-20 17:38:00         4 2017-12-20 17:20:00   \n",
      "1  desktop 2018-02-19 17:21:00         2 2018-02-19 16:53:00   \n",
      "2    touch 2017-07-01 01:54:00         5 2017-07-01 01:54:00   \n",
      "3  desktop 2018-05-20 11:23:00         9 2018-05-20 10:59:00   \n",
      "4  desktop 2017-12-27 14:06:00         3 2017-12-27 14:06:00   \n",
      "\n",
      "                    uid  \n",
      "0  16879256277535980062  \n",
      "1    104060357244891740  \n",
      "2   7459035603376831527  \n",
      "3  16174680259334210214  \n",
      "4   9969694820036681168  \n"
     ]
    }
   ],
   "source": [
    "# The Visits Table\n",
    "file_path = 'datasets_dirty/visits_log_us.csv'\n",
    "\n",
    "try:\n",
    "    open(file_path, 'r')\n",
    "except FileNotFoundError:\n",
    "    file_path = '/datasets/visits_log_us.csv'\n",
    "\n",
    "v_df = pd.read_csv(\n",
    "    file_path,\n",
    "    parse_dates=['Start Ts', 'End Ts'],\n",
    "    dtype=\n",
    "        {\n",
    "            'Device': 'category',\n",
    "            'Source Id': 'category'\n",
    "        }\n",
    ")\n",
    "\n",
    "v_df = v_df.rename(\n",
    "    columns={\n",
    "        'Uid': 'uid',\n",
    "        'Device': 'device',\n",
    "        'Start Ts': 'start_time',\n",
    "        'End Ts': 'end_time',\n",
    "        'Source Id': 'source_id'\n",
    "    }\n",
    ")\n",
    "check_data(v_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going from memory usage: 79.3 MB to memory usage: 8.9 MB without any loss of data? Nice.\n",
    "## The visits table (server logs with data on website visits):\n",
    "- uid — user's unique identifier\n",
    "    - Change from 'Uid' to 'uid'\n",
    "- device — user's device\n",
    "    - Change from 'Device' to 'device'\n",
    "    - There's only two different values, so I'll change the type to category\n",
    "- start_time — session start date and time\n",
    "    - Change name from 'Start Ts' to 'start_time'\n",
    "    - Looks like the seconds aren't included in this, I'll convert to datetime\n",
    "- end_time — session end date and time\n",
    "    - Change name from 'End Ts' to 'end_time'\n",
    "    - Change to datetime type also\n",
    "- source_id — identifier of the ad source the user came from\n",
    "    - Change name from 'Source Id' to 'source_id'\n",
    "    - There's only 10 unique values, so I changed this to category type. I'll come back and undo if I need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50415 entries, 0 to 50414\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   purchase_time  50415 non-null  datetime64[ns]\n",
      " 1   profit         50415 non-null  float64       \n",
      " 2   uid            50415 non-null  uint64        \n",
      "dtypes: datetime64[ns](1), float64(1), uint64(1)\n",
      "memory usage: 1.2 MB\n",
      "        purchase_time  profit                   uid\n",
      "0 2017-06-01 00:10:00   17.00  10329302124590727494\n",
      "1 2017-06-01 00:25:00    0.55  11627257723692907447\n",
      "2 2017-06-01 00:27:00    0.37  17903680561304213844\n",
      "3 2017-06-01 00:29:00    0.55  16109239769442553005\n",
      "4 2017-06-01 07:58:00    0.37  14200605875248379450\n"
     ]
    }
   ],
   "source": [
    "# The Orders Table\n",
    "file_path = 'datasets_dirty/orders_log_us.csv'\n",
    "\n",
    "try:\n",
    "    open(file_path, 'r')\n",
    "except FileNotFoundError:\n",
    "    file_path = '/datasets/orders_log_us.csv'\n",
    "\n",
    "o_df = pd.read_csv(\n",
    "    file_path,\n",
    "    parse_dates=['Buy Ts']\n",
    ")\n",
    "\n",
    "o_df = o_df.rename(\n",
    "    columns={\n",
    "        'Uid': 'uid',\n",
    "        'Buy Ts': 'purchase_time',\n",
    "        'Revenue': 'profit'\n",
    "    }\n",
    ")\n",
    "check_data(o_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going from memory usage: 4.4 MB to memory usage: 1.2 MB without any loss of data? Nice.\n",
    "## The orders table (data on orders):\n",
    "- uid — unique identifier of the user making an order\n",
    "    - Change from 'Uid' to 'uid'\n",
    "- purchase_time — order date and time\n",
    "    - Change from 'Buy Ts' to 'purchase_time'\n",
    "    - Convert to datetime type\n",
    "- profit — Yandex.Afisha's revenue from the order\n",
    "    - Change from 'Revenue' to 'profit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2542 entries, 0 to 2541\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   source_id  2542 non-null   category      \n",
      " 1   date       2542 non-null   datetime64[ns]\n",
      " 2   costs      2542 non-null   float64       \n",
      "dtypes: category(1), datetime64[ns](1), float64(1)\n",
      "memory usage: 42.7 KB\n",
      "  source_id       date  costs\n",
      "0         1 2017-06-01  75.20\n",
      "1         1 2017-06-02  62.25\n",
      "2         1 2017-06-03  36.53\n",
      "3         1 2017-06-04  55.00\n",
      "4         1 2017-06-05  57.08\n"
     ]
    }
   ],
   "source": [
    "# The Costs Table\n",
    "file_path = 'datasets_dirty/costs_us.csv'\n",
    "\n",
    "try:\n",
    "    open(file_path, 'r')\n",
    "except FileNotFoundError:\n",
    "    file_path = '/datasets/costs_us.csv'\n",
    "\n",
    "c_df = pd.read_csv(\n",
    "    file_path,\n",
    "    parse_dates=['dt'],\n",
    "    dtype=\n",
    "        {\n",
    "            'Device': 'category',\n",
    "            'source_id': 'category'\n",
    "        }\n",
    ")\n",
    "\n",
    "c_df = c_df.rename(\n",
    "    columns={\n",
    "        'dt': 'date'\n",
    "    }\n",
    ")\n",
    "check_data(c_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going from memory usage: 206.2 KB to memory usage: 42.7 KB without any loss of data? Stellar move by me.\n",
    "## The costs table (data on marketing expenses):\n",
    "- source_id — ad source identifier\n",
    "    - There's only 7 unique values. Convert to category type\n",
    "- dt — date\n",
    "    - change from 'dt' to 'date'\n",
    "    - It only has dates, and no times. Convert to datetime type accordingly\n",
    "- costs — expenses on this ad source on this day\n",
    "    - This looks fine unchanged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "```\n",
    "Q: How many people use this every day, week, and month?\n",
    "\n",
    "A: \n",
    "    Monthly average users =     23,228\n",
    "    Weekly average users =      5,716\n",
    "    Daily average users =       907 (about 15.88% of the weekly and 3.91% of the monthly users)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device</th>\n",
       "      <th>end_time</th>\n",
       "      <th>source_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>uid</th>\n",
       "      <th>session_year</th>\n",
       "      <th>session_month</th>\n",
       "      <th>session_week</th>\n",
       "      <th>session_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213585</th>\n",
       "      <td>desktop</td>\n",
       "      <td>2017-10-26 21:15:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-10-26 21:13:00</td>\n",
       "      <td>2751553619528980168</td>\n",
       "      <td>1970-01-01 00:00:00.000002017</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>2017-43</td>\n",
       "      <td>2017-10-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80067</th>\n",
       "      <td>desktop</td>\n",
       "      <td>2017-11-24 18:23:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-11-24 18:08:00</td>\n",
       "      <td>15787964388505675161</td>\n",
       "      <td>1970-01-01 00:00:00.000002017</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2017-47</td>\n",
       "      <td>2017-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293051</th>\n",
       "      <td>desktop</td>\n",
       "      <td>2017-11-22 12:29:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-22 12:27:00</td>\n",
       "      <td>12482367913018848776</td>\n",
       "      <td>1970-01-01 00:00:00.000002017</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2017-47</td>\n",
       "      <td>2017-11-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170862</th>\n",
       "      <td>desktop</td>\n",
       "      <td>2018-01-20 12:31:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-20 12:30:00</td>\n",
       "      <td>553499195425847886</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-02</td>\n",
       "      <td>2018-01-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179733</th>\n",
       "      <td>touch</td>\n",
       "      <td>2018-03-27 13:29:00</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-03-27 13:24:00</td>\n",
       "      <td>3820700620527732700</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2018-12</td>\n",
       "      <td>2018-03-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34450</th>\n",
       "      <td>desktop</td>\n",
       "      <td>2018-02-22 15:50:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-02-22 15:45:00</td>\n",
       "      <td>505460878143962777</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-07</td>\n",
       "      <td>2018-02-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262326</th>\n",
       "      <td>touch</td>\n",
       "      <td>2017-08-17 00:40:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-08-17 00:34:00</td>\n",
       "      <td>10264481331471440791</td>\n",
       "      <td>1970-01-01 00:00:00.000002017</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>2017-33</td>\n",
       "      <td>2017-08-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         device            end_time source_id          start_time  \\\n",
       "213585  desktop 2017-10-26 21:15:00         4 2017-10-26 21:13:00   \n",
       "80067   desktop 2017-11-24 18:23:00         2 2017-11-24 18:08:00   \n",
       "293051  desktop 2017-11-22 12:29:00         1 2017-11-22 12:27:00   \n",
       "170862  desktop 2018-01-20 12:31:00         4 2018-01-20 12:30:00   \n",
       "179733    touch 2018-03-27 13:29:00         9 2018-03-27 13:24:00   \n",
       "34450   desktop 2018-02-22 15:50:00         4 2018-02-22 15:45:00   \n",
       "262326    touch 2017-08-17 00:40:00         2 2017-08-17 00:34:00   \n",
       "\n",
       "                         uid                  session_year session_month  \\\n",
       "213585   2751553619528980168 1970-01-01 00:00:00.000002017    2017-10-01   \n",
       "80067   15787964388505675161 1970-01-01 00:00:00.000002017    2017-11-01   \n",
       "293051  12482367913018848776 1970-01-01 00:00:00.000002017    2017-11-01   \n",
       "170862    553499195425847886 1970-01-01 00:00:00.000002018    2018-01-01   \n",
       "179733   3820700620527732700 1970-01-01 00:00:00.000002018    2018-03-01   \n",
       "34450     505460878143962777 1970-01-01 00:00:00.000002018    2018-02-01   \n",
       "262326  10264481331471440791 1970-01-01 00:00:00.000002017    2017-08-01   \n",
       "\n",
       "       session_week session_date  \n",
       "213585      2017-43   2017-10-26  \n",
       "80067       2017-47   2017-11-24  \n",
       "293051      2017-47   2017-11-22  \n",
       "170862      2018-02   2018-01-20  \n",
       "179733      2018-12   2018-03-27  \n",
       "34450       2018-07   2018-02-22  \n",
       "262326      2017-33   2017-08-17  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add columns indicating the year, month, week, and day of a users `start_time`\n",
    "v_df['session_year']  = pd.to_datetime(v_df['start_time'].dt.isocalendar().year)\n",
    "v_df['session_month'] = pd.to_datetime(v_df['start_time'].dt.to_period('M').dt.to_timestamp()) # Retains year and month\n",
    "v_df['session_week'] = v_df['start_time'].dt.strftime('%Y-%U')  # Retains year and week\n",
    "v_df['session_date'] = pd.to_datetime(v_df['start_time'].dt.date)\n",
    "\n",
    "# v_df.sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dau = 907 wau = 5612 mau = 23228\n",
      "sticky_wau = 16.18% sticky_mau = 3.91%\n"
     ]
    }
   ],
   "source": [
    "# Calculating average daily, weekly, and monthly users\n",
    "dau_total = (\n",
    "    v_df.groupby('session_date')\n",
    "    .agg({'uid': 'nunique'})\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "wau_total = (\n",
    "    v_df.groupby(['session_year', 'session_week'])\n",
    "    .agg({'uid': 'nunique'})\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "mau_total = (\n",
    "    v_df.groupby(['session_year', 'session_month'])\n",
    "    .agg({'uid': 'nunique'})\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# Also, sticky versions\n",
    "sticky_wau = (dau_total / wau_total) * 100\n",
    "sticky_mau = (dau_total / mau_total) * 100\n",
    "\n",
    "print(f'dau = {int(dau_total)} wau = {int(wau_total)} mau = {int(mau_total)}')\n",
    "print(f'sticky_wau = {float(sticky_wau):.2f}% sticky_mau = {float(sticky_mau):.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: How many sessions are there per day?\n",
    "\n",
    "A: Average Daily Sessions = 987.36 sessions\n",
    "\n",
    "Q: What is the length of each session?\n",
    "\n",
    "A: Average Session length = 10.73mins\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Session length 10.73mins\n"
     ]
    }
   ],
   "source": [
    "# Finding session duration in seconds, then dividing by 60 to get mins, then print the average\n",
    "v_df['session_duration_mins'] = (\n",
    "    v_df['end_time'] - v_df['start_time']\n",
    ").dt.seconds / 60\n",
    "\n",
    "print(f\"Average Session length {float(v_df['session_duration_mins'].mean()):.2f}mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Daily Sessions 987.36 sessions\n"
     ]
    }
   ],
   "source": [
    "# Counting number of sessions per day, then printing the average\n",
    "v_by_day = v_df.groupby('session_date').agg({\n",
    "    'start_time': 'count'\n",
    "})\n",
    "v_by_day = v_by_day.rename(\n",
    "    columns={\n",
    "        'start_time': 'sessions'\n",
    "    }\n",
    ")\n",
    "print(f\"Average Daily Sessions {float(v_by_day['sessions'].mean()):.2f} sessions\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: What's the user retention rate?\n",
    "\n",
    "A: See the Pivot Table below\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating first session date to be used for other cohort calculations\n",
    "first_session_date = v_df.groupby(['uid'])['session_date'].min()\n",
    "first_session_date.name = 'first_session_date'\n",
    "\n",
    "v_df = v_df.join(first_session_date, on='uid')\n",
    "\n",
    "# For daily cohorts, if we want to later\n",
    "\n",
    "# v_df['cohort_lifetime_days'] = ((\n",
    "#         v_df['session_date']\n",
    "#             -\n",
    "#         v_df['first_session_date']\n",
    "#     ) / np.timedelta64(1, 'D')\n",
    "# ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For weekly cohorts, if we want to later\n",
    "\n",
    "# first_session_week = v_df.groupby(['uid'])['session_week'].min()\n",
    "# first_session_week.name = 'first_session_week'\n",
    "\n",
    "# v_df = v_df.join(first_session_week, on='uid')\n",
    "\n",
    "# v_df['cohort_lifetime_weeks'] = ((\n",
    "#         v_df['session_date']\n",
    "#             -\n",
    "#         v_df['first_session_date']\n",
    "#     ) / np.timedelta64(1, 'W')\n",
    "# ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 359400 entries, 0 to 359399\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count   Dtype         \n",
      "---  ------                  --------------   -----         \n",
      " 0   device                  359400 non-null  category      \n",
      " 1   end_time                359400 non-null  datetime64[ns]\n",
      " 2   source_id               359400 non-null  category      \n",
      " 3   start_time              359400 non-null  datetime64[ns]\n",
      " 4   uid                     359400 non-null  uint64        \n",
      " 5   session_year            359400 non-null  datetime64[ns]\n",
      " 6   session_month           359400 non-null  datetime64[ns]\n",
      " 7   session_week            359400 non-null  object        \n",
      " 8   session_date            359400 non-null  datetime64[ns]\n",
      " 9   session_duration_mins   359400 non-null  float64       \n",
      " 10  first_session_date      359400 non-null  datetime64[ns]\n",
      " 11  first_session_month     359400 non-null  datetime64[ns]\n",
      " 12  cohort_lifetime_months  359400 non-null  int32         \n",
      "dtypes: category(2), datetime64[ns](7), float64(1), int32(1), object(1), uint64(1)\n",
      "memory usage: 29.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>session_year</th>\n",
       "      <th>session_month</th>\n",
       "      <th>session_date</th>\n",
       "      <th>first_session_month</th>\n",
       "      <th>cohort_lifetime_months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-20 17:20:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002017</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2017-12-20</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-19 16:53:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-02-19</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-01 01:54:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002017</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-05-20 10:59:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-05-20</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-27 14:06:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002017</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>2017-12-27</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-09-03 21:35:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002017</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2017-09-03</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-01-30 11:13:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-01-30</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-11-05 15:14:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002017</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2017-11-05</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-07-19 10:41:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002017</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2017-07-19</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-11-08 13:42:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002017</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2017-11-08</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-07-14 12:43:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002017</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2017-07-14</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-01-25 19:37:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-01-25</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-02-06 15:45:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-11-27 14:42:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002017</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-04-23 18:00:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>2018-04-23</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-02-12 19:24:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>2018-02-12</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-05-14 08:39:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-05-30 08:49:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-05-30</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-04-05 16:28:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>2018-04-05</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-05-17 18:10:00</td>\n",
       "      <td>1970-01-01 00:00:00.000002018</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>2018-05-17</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            start_time                  session_year session_month  \\\n",
       "0  2017-12-20 17:20:00 1970-01-01 00:00:00.000002017    2017-12-01   \n",
       "1  2018-02-19 16:53:00 1970-01-01 00:00:00.000002018    2018-02-01   \n",
       "2  2017-07-01 01:54:00 1970-01-01 00:00:00.000002017    2017-07-01   \n",
       "3  2018-05-20 10:59:00 1970-01-01 00:00:00.000002018    2018-05-01   \n",
       "4  2017-12-27 14:06:00 1970-01-01 00:00:00.000002017    2017-12-01   \n",
       "5  2017-09-03 21:35:00 1970-01-01 00:00:00.000002017    2017-09-01   \n",
       "6  2018-01-30 11:13:00 1970-01-01 00:00:00.000002018    2018-01-01   \n",
       "7  2017-11-05 15:14:00 1970-01-01 00:00:00.000002017    2017-11-01   \n",
       "8  2017-07-19 10:41:00 1970-01-01 00:00:00.000002017    2017-07-01   \n",
       "9  2017-11-08 13:42:00 1970-01-01 00:00:00.000002017    2017-11-01   \n",
       "10 2017-07-14 12:43:00 1970-01-01 00:00:00.000002017    2017-07-01   \n",
       "11 2018-01-25 19:37:00 1970-01-01 00:00:00.000002018    2018-01-01   \n",
       "12 2018-02-06 15:45:00 1970-01-01 00:00:00.000002018    2018-02-01   \n",
       "13 2017-11-27 14:42:00 1970-01-01 00:00:00.000002017    2017-11-01   \n",
       "14 2018-04-23 18:00:00 1970-01-01 00:00:00.000002018    2018-04-01   \n",
       "15 2018-02-12 19:24:00 1970-01-01 00:00:00.000002018    2018-02-01   \n",
       "16 2018-05-14 08:39:00 1970-01-01 00:00:00.000002018    2018-05-01   \n",
       "17 2018-05-30 08:49:00 1970-01-01 00:00:00.000002018    2018-05-01   \n",
       "18 2018-04-05 16:28:00 1970-01-01 00:00:00.000002018    2018-04-01   \n",
       "19 2018-05-17 18:10:00 1970-01-01 00:00:00.000002018    2018-05-01   \n",
       "\n",
       "   session_date first_session_month  cohort_lifetime_months  \n",
       "0    2017-12-20          2017-12-01                       0  \n",
       "1    2018-02-19          2018-02-01                       0  \n",
       "2    2017-07-01          2017-07-01                       0  \n",
       "3    2018-05-20          2018-03-01                       2  \n",
       "4    2017-12-27          2017-12-01                       0  \n",
       "5    2017-09-03          2017-09-01                       0  \n",
       "6    2018-01-30          2017-06-01                       7  \n",
       "7    2017-11-05          2017-11-01                       0  \n",
       "8    2017-07-19          2017-07-01                       0  \n",
       "9    2017-11-08          2017-11-01                       0  \n",
       "10   2017-07-14          2017-07-01                       0  \n",
       "11   2018-01-25          2018-01-01                       0  \n",
       "12   2018-02-06          2017-06-01                       8  \n",
       "13   2017-11-27          2017-11-01                       0  \n",
       "14   2018-04-23          2017-10-01                       6  \n",
       "15   2018-02-12          2018-02-01                       0  \n",
       "16   2018-05-14          2018-05-01                       0  \n",
       "17   2018-05-30          2017-07-01                      10  \n",
       "18   2018-04-05          2018-04-01                       0  \n",
       "19   2018-05-17          2017-09-01                       7  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating Monthly Cohort Lifetime in Months\n",
    "\n",
    "first_session_month = v_df.groupby(['uid'])['session_month'].min()\n",
    "first_session_month.name = 'first_session_month'\n",
    "\n",
    "try:\n",
    "    v_df = v_df.join(first_session_month, on='uid')\n",
    "except:\n",
    "    print('we already have first_session_month')\n",
    "\n",
    "v_df['cohort_lifetime_months'] = (\n",
    "    (\n",
    "        v_df['session_date'].astype('datetime64[M]')\n",
    "            -\n",
    "        v_df['first_session_date'].astype('datetime64[M]')\n",
    "    ) / np.timedelta64(1, 'M')\n",
    ").astype(int)\n",
    "\n",
    "# v_df.info()\n",
    "# v_df[['start_time', 'session_year', 'session_month', 'session_date', 'first_session_month', 'cohort_lifetime_months']].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_df[['uid', 'cohort_lifetime_days', 'cohort_lifetime_weeks', 'cohort_lifetime_months']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cohorts_weekly = (\n",
    "#     v_df.groupby(['first_session_week', 'cohort_lifetime_weeks'])\n",
    "#     .agg({'uid': 'nunique'})\n",
    "#     .reset_index()\n",
    "# )\n",
    "\n",
    "# initial_users_count = cohorts_weekly[cohorts_weekly['cohort_lifetime_weeks'] == 0][ ['first_session_week', 'uid'] ]\n",
    "# initial_users_count = initial_users_count.rename(\n",
    "#     columns={'uid': 'weekly_cohort_users'}\n",
    "# ) \n",
    "\n",
    "# cohorts_weekly = cohorts_weekly.merge(initial_users_count, on='first_session_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by Cohort\n",
    "cohorts_monthly = (\n",
    "    v_df.groupby(['first_session_month', 'cohort_lifetime_months'])\n",
    "    .agg({'uid': 'nunique'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "initial_users_count = cohorts_monthly[cohorts_monthly['cohort_lifetime_months'] == 0][ ['first_session_month', 'uid'] ]\n",
    "initial_users_count = initial_users_count.rename(\n",
    "    columns={'uid': 'monthly_cohort_users'}\n",
    ") \n",
    "\n",
    "cohorts_monthly = cohorts_monthly.merge(initial_users_count, on='first_session_month')\n",
    "\n",
    "# Add retention rate column\n",
    "cohorts_monthly['retention'] = cohorts_monthly['uid'] / cohorts_monthly['monthly_cohort_users']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>cohort_lifetime_months</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_session_month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-06-01</th>\n",
       "      <td>100.00%</td>\n",
       "      <td>5.87%</td>\n",
       "      <td>5.58%</td>\n",
       "      <td>6.71%</td>\n",
       "      <td>6.77%</td>\n",
       "      <td>6.84%</td>\n",
       "      <td>5.99%</td>\n",
       "      <td>5.50%</td>\n",
       "      <td>5.39%</td>\n",
       "      <td>4.68%</td>\n",
       "      <td>4.03%</td>\n",
       "      <td>3.27%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-01</th>\n",
       "      <td>100.00%</td>\n",
       "      <td>5.13%</td>\n",
       "      <td>5.60%</td>\n",
       "      <td>5.35%</td>\n",
       "      <td>5.68%</td>\n",
       "      <td>4.63%</td>\n",
       "      <td>4.72%</td>\n",
       "      <td>4.38%</td>\n",
       "      <td>3.09%</td>\n",
       "      <td>2.77%</td>\n",
       "      <td>1.74%</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-01</th>\n",
       "      <td>100.00%</td>\n",
       "      <td>6.15%</td>\n",
       "      <td>6.10%</td>\n",
       "      <td>5.80%</td>\n",
       "      <td>4.42%</td>\n",
       "      <td>4.20%</td>\n",
       "      <td>4.09%</td>\n",
       "      <td>3.07%</td>\n",
       "      <td>2.62%</td>\n",
       "      <td>1.32%</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-01</th>\n",
       "      <td>100.00%</td>\n",
       "      <td>6.77%</td>\n",
       "      <td>6.33%</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>3.96%</td>\n",
       "      <td>3.74%</td>\n",
       "      <td>2.86%</td>\n",
       "      <td>2.37%</td>\n",
       "      <td>1.07%</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-01</th>\n",
       "      <td>100.00%</td>\n",
       "      <td>6.27%</td>\n",
       "      <td>4.37%</td>\n",
       "      <td>3.83%</td>\n",
       "      <td>3.44%</td>\n",
       "      <td>2.51%</td>\n",
       "      <td>2.04%</td>\n",
       "      <td>1.05%</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01</th>\n",
       "      <td>100.00%</td>\n",
       "      <td>5.08%</td>\n",
       "      <td>4.16%</td>\n",
       "      <td>3.63%</td>\n",
       "      <td>2.72%</td>\n",
       "      <td>2.15%</td>\n",
       "      <td>1.09%</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>100.00%</td>\n",
       "      <td>4.44%</td>\n",
       "      <td>3.41%</td>\n",
       "      <td>2.35%</td>\n",
       "      <td>2.02%</td>\n",
       "      <td>0.98%</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>100.00%</td>\n",
       "      <td>4.33%</td>\n",
       "      <td>3.01%</td>\n",
       "      <td>2.20%</td>\n",
       "      <td>0.91%</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>100.00%</td>\n",
       "      <td>3.47%</td>\n",
       "      <td>2.27%</td>\n",
       "      <td>1.02%</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>100.00%</td>\n",
       "      <td>3.13%</td>\n",
       "      <td>1.47%</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>100.00%</td>\n",
       "      <td>1.99%</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01</th>\n",
       "      <td>100.00%</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "cohort_lifetime_months       0      1      2      3      4      5      6   \\\n",
       "first_session_month                                                         \n",
       "2017-06-01              100.00%  5.87%  5.58%  6.71%  6.77%  6.84%  5.99%   \n",
       "2017-07-01              100.00%  5.13%  5.60%  5.35%  5.68%  4.63%  4.72%   \n",
       "2017-08-01              100.00%  6.15%  6.10%  5.80%  4.42%  4.20%  4.09%   \n",
       "2017-09-01              100.00%  6.77%  6.33%  4.14%  3.96%  3.74%  2.86%   \n",
       "2017-10-01              100.00%  6.27%  4.37%  3.83%  3.44%  2.51%  2.04%   \n",
       "2017-11-01              100.00%  5.08%  4.16%  3.63%  2.72%  2.15%  1.09%   \n",
       "2017-12-01              100.00%  4.44%  3.41%  2.35%  2.02%  0.98%          \n",
       "2018-01-01              100.00%  4.33%  3.01%  2.20%  0.91%                 \n",
       "2018-02-01              100.00%  3.47%  2.27%  1.02%                        \n",
       "2018-03-01              100.00%  3.13%  1.47%                               \n",
       "2018-04-01              100.00%  1.99%                                      \n",
       "2018-05-01              100.00%                                             \n",
       "\n",
       "cohort_lifetime_months     7      8      9      10     11  \n",
       "first_session_month                                        \n",
       "2017-06-01              5.50%  5.39%  4.68%  4.03%  3.27%  \n",
       "2017-07-01              4.38%  3.09%  2.77%  1.74%         \n",
       "2017-08-01              3.07%  2.62%  1.32%                \n",
       "2017-09-01              2.37%  1.07%                       \n",
       "2017-10-01              1.05%                              \n",
       "2017-11-01                                                 \n",
       "2017-12-01                                                 \n",
       "2018-01-01                                                 \n",
       "2018-02-01                                                 \n",
       "2018-03-01                                                 \n",
       "2018-04-01                                                 \n",
       "2018-05-01                                                 "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make and print table\n",
    "retention_pivot = cohorts_monthly.pivot_table(\n",
    "    index='first_session_month',\n",
    "    columns='cohort_lifetime_months',\n",
    "    values='retention',\n",
    "    aggfunc='sum'\n",
    ")\n",
    "\n",
    "# Format the percents\n",
    "retention_pivot = retention_pivot.applymap(lambda x: '{:.2%}'.format(x))\n",
    "\n",
    "# Get rid og the nan% values\n",
    "retention_pivot = retention_pivot.replace('nan%', '')\n",
    "\n",
    "retention_pivot\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: When do people start buying?\n",
    "\n",
    "A:Average Conversion Rate = 16.90 days\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>first_session_date</th>\n",
       "      <th>first_purchase_date</th>\n",
       "      <th>conversion_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36374</th>\n",
       "      <td>18366944480640554137</td>\n",
       "      <td>2017-12-10</td>\n",
       "      <td>2017-12-25 16:27:00</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31692</th>\n",
       "      <td>16004931378838924750</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>2017-06-29 18:39:00</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13651</th>\n",
       "      <td>6840944448800081883</td>\n",
       "      <td>2017-10-09</td>\n",
       "      <td>2017-10-11 23:19:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12949</th>\n",
       "      <td>6461972137199349313</td>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>2017-12-13 20:23:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12280</th>\n",
       "      <td>6140503418397349832</td>\n",
       "      <td>2017-08-05</td>\n",
       "      <td>2017-10-06 08:51:00</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16930</th>\n",
       "      <td>8512581799131126070</td>\n",
       "      <td>2018-04-15</td>\n",
       "      <td>2018-04-16 07:02:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6114</th>\n",
       "      <td>3091456611554355756</td>\n",
       "      <td>2018-03-05</td>\n",
       "      <td>2018-03-21 09:37:00</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21730</th>\n",
       "      <td>10976909326931315696</td>\n",
       "      <td>2017-11-20</td>\n",
       "      <td>2017-12-07 15:21:00</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>489376571683475084</td>\n",
       "      <td>2017-10-10</td>\n",
       "      <td>2017-12-06 10:56:00</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34640</th>\n",
       "      <td>17486284766854249763</td>\n",
       "      <td>2017-11-24</td>\n",
       "      <td>2017-11-24 18:06:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        uid first_session_date first_purchase_date  \\\n",
       "36374  18366944480640554137         2017-12-10 2017-12-25 16:27:00   \n",
       "31692  16004931378838924750         2017-06-01 2017-06-29 18:39:00   \n",
       "13651   6840944448800081883         2017-10-09 2017-10-11 23:19:00   \n",
       "12949   6461972137199349313         2017-12-13 2017-12-13 20:23:00   \n",
       "12280   6140503418397349832         2017-08-05 2017-10-06 08:51:00   \n",
       "16930   8512581799131126070         2018-04-15 2018-04-16 07:02:00   \n",
       "6114    3091456611554355756         2018-03-05 2018-03-21 09:37:00   \n",
       "21730  10976909326931315696         2017-11-20 2017-12-07 15:21:00   \n",
       "990      489376571683475084         2017-10-10 2017-12-06 10:56:00   \n",
       "34640  17486284766854249763         2017-11-24 2017-11-24 18:06:00   \n",
       "\n",
       "       conversion_days  \n",
       "36374               15  \n",
       "31692               28  \n",
       "13651                2  \n",
       "12949                0  \n",
       "12280               62  \n",
       "16930                1  \n",
       "6114                16  \n",
       "21730               17  \n",
       "990                 57  \n",
       "34640                0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the earliest purchase date for each uid (aka for each user)\n",
    "first_purchase_date = o_df.groupby(['uid'])['purchase_time'].min()\n",
    "first_purchase_date.name = 'first_purchase_date'\n",
    "\n",
    "user_conversion = (\n",
    "    first_session_date.to_frame()\n",
    "    .join(first_purchase_date, on='uid')\n",
    "    .dropna()\n",
    "    # .fillna('never')\n",
    ")\n",
    "user_conversion.reset_index(inplace=True)\n",
    "\n",
    "# Add number of days between users first session and first purchase\n",
    "user_conversion['conversion_days'] = ((\n",
    "        user_conversion['first_purchase_date']\n",
    "            -\n",
    "        pd.to_datetime(user_conversion['first_session_date'])\n",
    "    ) / np.timedelta64(1, 'D')\n",
    ").astype(int)\n",
    "\n",
    "# user_conversion.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Conversion Rate 16.90 days\n"
     ]
    }
   ],
   "source": [
    "# user_conversion['conversion_days'].value_counts()\n",
    "print(f\"Average Conversion Rate {float(user_conversion['conversion_days'].mean()):.2f} days\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Q: How many orders do they make during a given period of time?\n",
    "\n",
    "A: \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df[[\n",
    "    'uid', \n",
    "    'cohort_lifetime_months', \n",
    "    'cohort_lifetime_weeks', \n",
    "    'cohort_lifetime_days'\n",
    "]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_df = o_df.merge(user_conversion[['uid', 'first_purchase_date']], on='uid', how='left')\n",
    "\n",
    "# o_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_df['user_age'] = ((\n",
    "        o_df['purchase_time']\n",
    "            -\n",
    "        o_df['first_purchase_date']\n",
    "    ) / np.timedelta64(1, 'D')\n",
    ")\n",
    "\n",
    "# o_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group o_df by uid and calculate the total number of purchases per uid\n",
    "purchase_count = o_df.groupby('uid')['purchase_time'].count()\n",
    "\n",
    "# Calculate the duration in days, weeks, and months for each uid\n",
    "duration_days = (o_df.groupby('uid')['purchase_time'].max() - o_df.groupby('uid')['purchase_time'].min()).dt.days\n",
    "duration_weeks = duration_days / 7\n",
    "duration_months = duration_days / 30\n",
    "\n",
    "# Calculate the average number of purchases per day, week, and month\n",
    "avg_purchases_per_day = np.where(duration_days == 0, 0, purchase_count / duration_days)\n",
    "avg_purchases_per_week = np.where(duration_weeks == 0, 0, purchase_count / duration_weeks)\n",
    "avg_purchases_per_month = np.where(duration_months == 0, 0, purchase_count / duration_months)\n",
    "\n",
    "# Combine the results into a new DataFrame\n",
    "average_purchases_df = pd.DataFrame({\n",
    "    'uid': purchase_count.index,\n",
    "    'avg_purchases_per_day': avg_purchases_per_day,\n",
    "    'avg_purchases_per_week': avg_purchases_per_week,\n",
    "    'avg_purchases_per_month': avg_purchases_per_month\n",
    "})\n",
    "\n",
    "# Display the average purchases per day, week, and month\n",
    "# average_purchases_df.sample(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group o_df by uid and calculate the total profit and number of purchases per uid\n",
    "purchase_stats = o_df.groupby('uid').agg({'profit': 'sum', 'purchase_time': 'count'})\n",
    "\n",
    "# Calculate the average purchase size per user\n",
    "average_purchase_size_per_user = purchase_stats['profit'] / purchase_stats['purchase_time']\n",
    "\n",
    "# Add the average purchase size per user to o_df\n",
    "o_df = o_df.merge(average_purchase_size_per_user.rename('average_purchase_size_per_user'), on='uid')\n",
    "\n",
    "# Display the average purchase size per user\n",
    "average_purchase_size_per_user_df = o_df[['uid', 'average_purchase_size_per_user']].drop_duplicates()\n",
    "# average_purchase_size_per_user_df.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ltv_data = o_df.groupby('uid')['profit'].sum().reset_index()\n",
    "\n",
    "total_ltv_data = total_ltv_data.sort_values('profit', ascending=False)\n",
    "\n",
    "total_ltv_data.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_stats = o_df.groupby('uid').agg({'profit': 'sum', 'purchase_time': 'count'})\n",
    "\n",
    "purchase_stats.sample(20)\n",
    "\n",
    "average_purchase_size = purchase_stats['profit'] / purchase_stats['purchase_time']\n",
    "\n",
    "# print(average_purchase_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_costs = c_df.agg({'costs': 'sum'})\n",
    "total_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcely_costs = c_df.groupby('source_id').agg({'costs':'sum'}).reset_index()\n",
    "\n",
    "# sourcely_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_costs = c_df.groupby('date').agg({'costs': 'sum'})\n",
    "\n",
    "# Create the plot using Pandas plot()\n",
    "daily_costs.plot(figsize=(10, 6))\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Costs by Date')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Costs')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcely_users = (\n",
    "    v_df.groupby(['source_id'])\n",
    "    .agg({'uid': 'nunique'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "sourcely_users.columns = ['source_id', 'n_users']\n",
    "\n",
    "# sourcely_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac_report = pd.merge(\n",
    "    sourcely_costs,\n",
    "    sourcely_users,\n",
    "    on='source_id'\n",
    ")\n",
    "# cac_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cac_report['cac'] = cac_report['costs'] / cac_report['n_users']\n",
    "# cac_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the average CAC\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(cac_report['source_id'], cac_report['cac'])\n",
    "plt.xlabel('Source ID')\n",
    "plt.ylabel('Average CAC')\n",
    "plt.title('Average Customer Acquisition Cost by Source ID')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ltv_per_user = (\n",
    "    total_ltv_data.agg({'profit':'mean'})\n",
    "    .reset_index()\n",
    ")\n",
    "avg_ltv_per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_n_users = (\n",
    "    v_df.agg({'uid': 'nunique'})\n",
    "    .reset_index()\n",
    ")\n",
    "total_n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_cac_per_user = (total_costs[0] / total_n_users[0]).reset_index()\n",
    "average_cac_per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = (avg_ltv_per_user[0] / average_cac_per_user[0]).reset_index()\n",
    "\n",
    "print('roi =', roi[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph CAC, LTV, and ROI per device per week\n",
    "\n",
    "# Calculate CAC per source_id and time period\n",
    "cac_per_source = c_df.groupby(['source_id', pd.Grouper(key='date', freq='W-SUN')]).agg({'costs': 'sum'}).reset_index()\n",
    "\n",
    "# Calculate LTV per source_id and time period\n",
    "# Merge visits and orders tables on 'uid' column\n",
    "v_and_o_dfs = v_df.merge(o_df, on='uid')\n",
    "\n",
    "# Group merged dataframe by 'source_id' and week of 'purchase_time', calculate LTV\n",
    "sourcely_ltv = v_and_o_dfs.groupby(['source_id', pd.Grouper(key='purchase_time', freq='W-SUN')]).agg(\n",
    "    ltv=('profit', 'sum'),\n",
    "    unique_users=('uid', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate ROI per source_id and time period\n",
    "roi_per_source = sourcely_ltv.copy()\n",
    "roi_per_source['roi'] = roi_per_source['ltv'] / cac_per_source['costs']\n",
    "\n",
    "# Plotting CAC per source_id\n",
    "plt.figure(figsize=(10, 6))\n",
    "for source_id, group in cac_per_source.groupby('source_id'):\n",
    "    plt.plot(group['date'], group['costs'], label=f\"Source {source_id}\")\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('CAC')\n",
    "plt.title('CAC per Source over Time (Weekly)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting LTV per source_id\n",
    "plt.figure(figsize=(10, 6))\n",
    "for source_id, group in sourcely_ltv.groupby('source_id'):\n",
    "    plt.plot(group['purchase_time'], group['ltv'], label=f\"Source {source_id}\")\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('LTV')\n",
    "plt.title('LTV per Source over Time (Weekly)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting ROI per source_id\n",
    "plt.figure(figsize=(10, 6))\n",
    "for source_id, group in roi_per_source.groupby('source_id'):\n",
    "    plt.plot(group['purchase_time'], group['roi'], label=f\"Source {source_id}\")\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('ROI')\n",
    "plt.title('ROI per Source over Time (Weekly)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph CAC, LTV, and ROI per device per week\n",
    "\n",
    "# Extract the date part from the \"end_time\" column in v_df\n",
    "v_df['date'] = pd.to_datetime(v_df['end_time'].dt.date)\n",
    "\n",
    "# Merge v_df and c_df based on the date part\n",
    "v_and_c_dfs = v_df.merge(c_df, on='date')\n",
    "\n",
    "# Calculate CAC per device and week\n",
    "cac_per_device_week = v_and_c_dfs.groupby([v_and_c_dfs['device'], pd.Grouper(key='date', freq='W-MON')])['costs'].sum().reset_index()\n",
    "\n",
    "# Calculate LTV per device and week\n",
    "\n",
    "# Calculate LTV per device and week\n",
    "ltv_per_device_week = v_and_o_dfs.groupby([v_and_o_dfs['device'], pd.Grouper(key='purchase_time', freq='W-SUN')])['profit'].sum().reset_index()\n",
    "\n",
    "# Calculate ROI per device and week\n",
    "roi_per_device_week = ltv_per_device_week.copy()\n",
    "roi_per_device_week['roi'] = roi_per_device_week['profit'] / cac_per_device_week['costs']\n",
    "\n",
    "# Plotting CAC per device\n",
    "plt.figure(figsize=(10, 6))\n",
    "for device, group in cac_per_device_week.groupby('device'):\n",
    "    plt.plot(group['date'], group['costs'], label=device)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('CAC')\n",
    "plt.title('CAC per Device over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting LTV per device\n",
    "plt.figure(figsize=(10, 6))\n",
    "for device, group in ltv_per_device_week.groupby('device'):\n",
    "    plt.plot(group['purchase_time'], group['profit'], label=device)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('LTV')\n",
    "plt.title('LTV per Device over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting ROI per device\n",
    "plt.figure(figsize=(10, 6))\n",
    "for device, group in roi_per_device_week.groupby('device'):\n",
    "    plt.plot(group['purchase_time'], group['roi'], label=device)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('ROI')\n",
    "plt.title('ROI per Device over Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
